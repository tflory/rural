{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T22:49:40.749216Z",
     "start_time": "2020-07-30T22:49:40.740317Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardcoded data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T22:49:40.771448Z",
     "start_time": "2020-07-30T22:49:40.764857Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['Company','Address Line 1','City','State','ZipCode','Employee Size (5) - Location',\n",
    "        'County Code','Sales Volume (9) - Location','Primary NAICS Code','NAICS8 Descriptions',\n",
    "        'Primary SIC Code','SIC6_Descriptions','Business Status Code', \n",
    "        'Year Established','Subsidiary Number', 'Parent Number', \n",
    "        'Parent Actual Employee Size','Parent Actual Sales Volume','ABI','Census Tract',\n",
    "        'Census Block','Latitude','Longitude','CBSA Code','CBSA Level','CSA Code','FIPS Code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T22:49:40.787849Z",
     "start_time": "2020-07-30T22:49:40.773620Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assign the FIPS state code to each record. Prior examination shows some data entry \n",
    "# errors in the FIPS Code variable so we do this by brute force.\n",
    "state_fips = {\n",
    "'AL':'01',\n",
    "'AK':'02',\n",
    "'AS':'60',\n",
    "'AZ':'04',\n",
    "'AR':'05',\n",
    "'CA':'06',\n",
    "'CO':'08',\n",
    "'CT':'09',\n",
    "'DE':'10',\n",
    "'DC':'11',\n",
    "'FL':'12',\n",
    "'FM':'64',\n",
    "'GA':'13',\n",
    "'GU':'66',\n",
    "'HI':'15',\n",
    "'ID':'16',\n",
    "'IL':'17',\n",
    "'IN':'18',\n",
    "'IA':'19',\n",
    "'KS':'20',\n",
    "'KY':'21',\n",
    "'LA':'22',\n",
    "'ME':'23',\n",
    "'MH':'68',\n",
    "'MD':'24',\n",
    "'MA':'25',\n",
    "'MI':'26',\n",
    "'MN':'27',\n",
    "'MS':'28',\n",
    "'MO':'29',\n",
    "'MT':'30',\n",
    "'NE':'31',\n",
    "'NV':'32',\n",
    "'NH':'33',\n",
    "'NJ':'34',\n",
    "'NM':'35',\n",
    "'NY':'36',\n",
    "'NC':'37',\n",
    "'ND':'38',\n",
    "'MP':'69',\n",
    "'OH':'39',\n",
    "'OK':'40',\n",
    "'OR':'41',\n",
    "'PW':'70',\n",
    "'PA':'42',\n",
    "'PR':'72',\n",
    "'RI':'44',\n",
    "'SC':'45',\n",
    "'SD':'46',\n",
    "'TN':'47',\n",
    "'TX':'48',\n",
    "'UM':'74',\n",
    "'UT':'49',\n",
    "'VT':'50',\n",
    "'VA':'51',\n",
    "'VI':'78',\n",
    "'WA':'53',\n",
    "'WV':'54',\n",
    "'WI':'55',\n",
    "'WY':'56'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T22:49:40.805168Z",
     "start_time": "2020-07-30T22:49:40.790227Z"
    }
   },
   "outputs": [],
   "source": [
    "# Short descriptions for 2-digit NAICS codes\n",
    "naics_desc = {\n",
    "'11':'Agriculture, Forestry, Fishing and Hunting',\n",
    "'21':'Mining',\n",
    "'22':'Utilities',\n",
    "'23':'Construction',\n",
    "'31':'Manufacturing',\n",
    "'32':'Manufacturing',\n",
    "'33':'Manufacturing',\n",
    "'42':'Wholesale Trade',\n",
    "'44':'Retail Trade',\n",
    "'45':'Retail Trade',\n",
    "'48':'Transportation and Warehousing',\n",
    "'49':'Transportation and Warehousing',\n",
    "'51':'Information',\n",
    "'52':'Finance and Insurance',\n",
    "'53':'Real Estate Rental and Leasing',\n",
    "'54':'Professional, Scientific, and Technical Services',\n",
    "'55':'Management of Companies and Enterprises',\n",
    "'56':'Administrative and Support and Waste Management and Remediation Services',\n",
    "'61':'Eucational Services',\n",
    "'62':'Health Care and Social Assistance',\n",
    "'71':'Arts, Entertainment, and Recreation',\n",
    "'72':'Accommodation and Food Services',\n",
    "'81':'Other Services (except Public Administration)',\n",
    "'92':'Public Administration',\n",
    "'99':'Unknown'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T22:49:40.824123Z",
     "start_time": "2020-07-30T22:49:40.807832Z"
    }
   },
   "outputs": [],
   "source": [
    "ag_naics = {'112112':'Cattle Feedlots',\n",
    "            '115111':'Cotton Ginning',\n",
    "            '115112':'Soil Preparation, Planting and Cultivating',\n",
    "            '115113':'Crop Harvesting, Primarily by Machine',\n",
    "            '115114':'Postharvest Crop Activities except Cotton Ginning',\n",
    "            '115115':'Farm Labor Contractors and Crew Leaders',\n",
    "            '115116':'Farm Management Services',\n",
    "            '423820':'Farm and Garden Machinery and Equipment Merchant Wholesalers',\n",
    "            '424430':'Dairy Product Merchant Wholesalers',\n",
    "            '424440':'Poultry and Poultry Product Merchant Wholesalers',\n",
    "            '424480':'Fresh Fruit and Vegetable Merchant Wholesalers',\n",
    "            '424510':'Grain and Field Bean Merchant Wholesalers',\n",
    "            '424520':'Livestock Merchant Wholesalers',\n",
    "            '424590':'Other Farm Product Raw Material Merchant Wholesalers',\n",
    "            '424910':'Farm Supplies Merchant Wholesalers',\n",
    "            '493130':'Farm Product Warehousing and Storage'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T22:49:40.877349Z",
     "start_time": "2020-07-30T22:49:40.827042Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_and_correct():\n",
    "    print(f'\\n{yr}:',file=logfile)\n",
    "    dir = '/InfoGroup/data/original/'\n",
    "    xdir = '/tmp/xtrcts/'\n",
    "    fname = f'{yr}_Business_Academic_QCQ_utf-8.'\n",
    "    with ZipFile(dir + fname + 'zip','r') as myzip:\n",
    "        myzip.extract(fname + 'csv',xdir)\n",
    "        df = pd.read_csv(xdir + fname + 'csv',dtype=object,usecols=cols)\n",
    "        os.remove(xdir + fname + 'csv')\n",
    "    \n",
    "    # Overwrite the state FIPS code\n",
    "    df['State FIPS'] = df['State'].apply(state_code)\n",
    "    df['FIPS Code'] = df.apply(new_fips, axis=1)\n",
    "\n",
    "    # Exclude territories, leaving only the 50 states and DC.\n",
    "    df['State FIPS'] = df['State FIPS'].astype(int)\n",
    "    df = df[df['State FIPS'] <= 56]\n",
    "        \n",
    "    # Flag the 48 continental states\n",
    "    df['Continental'] = df['State FIPS'].apply(is_continental)\n",
    "        \n",
    "    # Give 'CBSA Level' a missing value of 0, meaning 'rural'\n",
    "    df['CBSA Level'].fillna('0',inplace=True)\n",
    "        \n",
    "     # Give \"Primary NAICS Code\" a useable missing value code.\n",
    "    df['Primary NAICS Code'].fillna('99999999',inplace=True)\n",
    "        \n",
    "    # Create 2-digit NAICS code\n",
    "    df['NAICS2'] = df['Primary NAICS Code'].apply(lambda x: str(x)[:2])\n",
    "        \n",
    "    # Create descriptions of the 2-digit NAICS codes\n",
    "    df['NAICS2 desc'] = df['NAICS2'].apply(naics2_desc)\n",
    "\n",
    "    # The project statement mentions some 6-digit NAICS codes as\n",
    "    # definitely referring to agricultural activities.\n",
    "    # Create 6-digit versions of all NAICS code and descriptions for \n",
    "    # those specified in the project statement.\n",
    "    df['NAICS6'] = df['Primary NAICS Code'].apply(lambda x: str(x)[:6])\n",
    "    df['NAICS6 desc'] = df['NAICS6'].apply(naics6_desc)    \n",
    "    return df\n",
    "\n",
    "def CBSA_partition(df):\n",
    "    df['CBSA Code'].fillna(-9,inplace=True)\n",
    "    df['CBSA Level'].fillna(-9,inplace=True)\n",
    "    df['CBSA Code'] = df['CBSA Code'].astype(int)\n",
    "    df['CBSA Level'] = df['CBSA Level'].astype(int)\n",
    "    df['FIPS Code'] = df['FIPS Code'].astype(int)\n",
    "    \n",
    "    urban = df[(df['CBSA Level'] > 0)]\n",
    "    urban['CBSA Text'] = 'urban'\n",
    "    rural = df[(df['CBSA Code'] > 0) & (df['CBSA Level']==0)]\n",
    "    rural['CBSA Text'] ='rural'\n",
    "    unknown = df[(df['CBSA Code'] < 0) | ((df['CBSA Code'] == 0) & (df['CBSA Level']==0))]\n",
    "    unknown['FIPS Code'] = unknown['FIPS Code'].astype(int)\n",
    "    \n",
    "    nrows = len(df)\n",
    "    sum_of_parts = len(urban) + len(rural) + len(unknown)\n",
    "    if sum_of_parts != nrows:\n",
    "        print(f'Error in dividing enterprises into categories:',file=logfile)\n",
    "        print(f'\\t{nrows} != {sum_of_parts}',file=logfile)\n",
    "        \n",
    "    corrected = extract_corrections(unknown)\n",
    "    corrected = cbsa_text(corrected,yr)\n",
    "    corrected.drop(columns=['CBSA','FIPS Code_r','LSAD'],inplace=True)\n",
    "    corrected.rename(columns={\"FIPS Code_l\": \"FIPS Code\"},inplace=True)\n",
    "\n",
    "    print(corrected['CBSA Level'].value_counts(),file=logfile)\n",
    "    return (urban, rural, corrected)\n",
    "\n",
    "def new_vars(df):\n",
    "    print(df['CBSA Text'].value_counts(),file=logfile)\n",
    "    print(df['CBSA Text'].value_counts(normalize=True) * 100,file=logfile)\n",
    "    print(f'rural/urban by OMB standard:',file=logfile)\n",
    "    \n",
    "    df['CBSA Code'] = df['CBSA Code'].fillna(99999)\n",
    "    df['CBSA Code'] = df['CBSA Code'].astype(int)\n",
    "    df['UA Code'] = df['CBSA Code'].apply(find_ua)\n",
    "    showtime('\\t5-0')\n",
    "    df['UA Type'] = df['UA Code'].apply(get_ua_type)\n",
    "    showtime('\\t5-1')\n",
    "    df['rural_by_UA'] = df['UA Code'].apply(how_rural) \n",
    "    showtime('\\t5-2')\n",
    "    print(df['UA Type'].value_counts(),file=logfile)\n",
    "    print(df['UA Type'].value_counts(normalize=True) * 100,file=logfile)\n",
    "    print(df['rural_by_UA'].value_counts(),file=logfile)\n",
    "    print(df['rural_by_UA'].value_counts(normalize=True) * 100,file=logfile)\n",
    "    \n",
    "    df['Census Tract'].fillna('999999',inplace=True)\n",
    "    df['FIPS Code'].fillna('99999',inplace=True)\n",
    "    df['Full Census Tract'] = df.apply(combo,axis=1)\n",
    "    showtime('\\t5-3')\n",
    "    df['rural_HRSA'] = df.apply(rur3,axis=1)\n",
    "    showtime('\\t5-4')\n",
    "    print(df['rural_HRSA'].value_counts(),file=logfile)\n",
    "    print(df['rural_HRSA'].value_counts(normalize=True) * 100,file=logfile)\n",
    "\n",
    "    df['ZipCode'] = df['ZipCode'].apply(lambda x: x.zfill(5) if len(x) < 5 == 0 else x)\n",
    "    showtime('\\t5-5')\n",
    "    return df\n",
    "\n",
    "def merges_and_output(df):\n",
    "    df_out = df.merge(df_zip,how='left',left_on='ZipCode',right_on='ZIP',indicator=True)\n",
    "    showtime('\\t6-0')\n",
    "    df_out.drop(columns=['ZIP','_merge'],inplace=True)\n",
    "    \n",
    "    # FAR codes apply only to the continental states. To select just those for reporting:\n",
    "    # df_select = df_out[(df_out['Continental']=='1') & (df_out['State FIPS'] != '11')].copy()\n",
    "    \n",
    "    # Log the distribution of FAR levels by year.\n",
    "    log_far_levels(df_out)\n",
    "\n",
    "    merged = df_out.merge(tracts,how='left',left_on='Full Census Tract',right_on='GEOID',indicator=True)\n",
    "    showtime('\\t6-1')\n",
    "    print('Rural=1,Urban=0:',file=logfile)\n",
    "    print(merged['rural_tract'].value_counts(),file=logfile)\n",
    "    merged.drop(columns=['GEOID','_merge'],inplace=True)\n",
    "    print('Missing:',file=logfile)\n",
    "    print(merged['rural_tract'].isnull().sum(),file=logfile)\n",
    "    merged.to_csv(f'/InfoGroup/data/rurality/InfoGroup_{yr}_all_steps.csv',index=None)\n",
    "    showtime('\\t6-2\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Little functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T22:49:40.933134Z",
     "start_time": "2020-07-30T22:49:40.879439Z"
    }
   },
   "outputs": [],
   "source": [
    "def state_code(state):\n",
    "    \"\"\" Establish an accurate state FIPS code\"\"\"\n",
    "    return state_fips[state]\n",
    "\n",
    "def new_fips(row):\n",
    "    if str(row['State FIPS']) == 'nan' or str(row['County Code']) == 'nan':\n",
    "        return '00000'\n",
    "    else:\n",
    "        return str(row['State FIPS']) + str(row['County Code'])\n",
    "\n",
    "def is_continental(code):\n",
    "    \"\"\" Create a 1/0 flag for the 48 continental states and DC, excluding HI and AK. \n",
    "        This could be useful for mapping.\n",
    "    \"\"\"\n",
    "    if int(code) not in [2,15]:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def naics2_desc(code):\n",
    "    \"\"\" Create a variable with text descriptions of the 2-digit NAICS codes\"\"\"\n",
    "    return naics_desc[code]\n",
    "\n",
    "def naics6_desc(code):\n",
    "    \"\"\"Assign text descriptions to certain 6-digit NAICS codes.\"\"\"\n",
    "    if code in ag_naics.keys():\n",
    "        return ag_naics[code]\n",
    "    return np.nan\n",
    "\n",
    "def extract_corrections(unknowns):\n",
    "    \"\"\"Extracts CBSA Code and appropriate CBSA Level for a list of InfoGroup FIPS Codes\"\"\"\n",
    "    # Files cross-references CBSA codes and county FIPS codes.\n",
    "    # Variable 'STCOU' is the 5-digit county FIPS code. The CBSA Level is inferred from the\n",
    "    # text in the 'LSAD' variable.\n",
    "    unk = unknowns.join(cbsa_df,on='FIPS Code',how='left',lsuffix='_l',rsuffix='_r')\n",
    "    unk['CBSA Level'] = 0\n",
    "    for i in unk.index:\n",
    "        if str(unk.at[i,'LSAD']).find(\"Metropolitan\") > -1:\n",
    "            unk.at[i,'CBSA Level'] = 2\n",
    "        elif str(unk.at[i,'LSAD']).find(\"Micropolitan\") > -1:\n",
    "            unk.at[i,'CBSA Level'] = 1\n",
    "    return unk\n",
    "\n",
    "def cbsa_text(df,yr):\n",
    "    \"\"\"Create a text description of he CBSA level code.\"\"\"\n",
    "    df['CBSA Text'] = ''\n",
    "    for i in df.index:\n",
    "        if int(df.at[i,'CBSA Level']) > 0: \n",
    "            df.at[i,'CBSA Text'] = 'urban'\n",
    "        elif df.at[i,'CBSA Code'] == -9 and df.at[i,'CBSA Level']==0:\n",
    "            df.at[i,'CBSA Text'] = 'rural'\n",
    "        elif (df.at[i,'CBSA Code']==0 and df.at[i,'CBSA Level']==0): \n",
    "            df.at[i,'CBSA Text'] = 'rural'\n",
    "        elif df.at[i,'CBSA Level']==0:\n",
    "            df.at[i,'CBSA Text'] = 'unknown'\n",
    "        else:\n",
    "            df.at[i,'CBSA Text'] = '????'\n",
    "    return df\n",
    "\n",
    "def all_nines(li):\n",
    "    \"\"\"Determine whether the list of CBSA codes in a UA are all missing values.\"\"\"\n",
    "    length = len(li)\n",
    "    n = 0\n",
    "    for item in li:\n",
    "        if item == 99999:\n",
    "            n += 1\n",
    "    if n == length:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def how_rural(code):\n",
    "    \"\"\"Apply the UA rurality flag.\"\"\"\n",
    "    if code in ua_keys.keys():\n",
    "        return ua_keys[code]\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def find_ua(code):\n",
    "    \"\"\"Get the UA code that matches a particular CBSA code in the census relationship file.\"\"\"\n",
    "    for pair in pairs:\n",
    "        if pair[1] == code:\n",
    "            return pair[0]\n",
    "\n",
    "def ua_type(place):\n",
    "    \"\"\"Create the UA Type value by skimming the last two words off the \n",
    "       relationship file's UANAME value.\n",
    "    \"\"\"\n",
    "    if place.find('Not in a 2010 urban area') != -1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        list = re.findall(\"(\\S+)\",place)\n",
    "        return ' '.join(list[len(list)-2:])\n",
    "\n",
    "def get_ua_type(code):\n",
    "    \"\"\"Return the UA Type value for a UA Code.\"\"\"\n",
    "    try:\n",
    "        return ua_type_dict[code]\n",
    "    except KeyError:\n",
    "        return np.nan\n",
    "\n",
    "def combo(row):\n",
    "    if row['FIPS Code'] == 99999 or row['Census Tract'] == 999999:\n",
    "        return np.nan\n",
    "    else:\n",
    "        #try:\n",
    "        return str(row['FIPS Code']) + str(row['Census Tract'])\n",
    "        #except TypeError:\n",
    "        #    fips = str(int(row['FIPS Code']))\n",
    "        #    tract = str(int(row['Census Tract']))\n",
    "        #    return fips + tract\n",
    "        \n",
    "def rur3(row):\n",
    "    if int(row['CBSA Level']) == 0 or row['Full Census Tract'] in rural_tracts:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def farlevel(row):\n",
    "    if sum([row['far1'],row['far2'],row['far3'],row['far4']]) == 0:\n",
    "        return 0\n",
    "    elif row['far1'] == 1 and sum([row['far2'],row['far3'],row['far4']]) == 0:\n",
    "        return 1\n",
    "    elif row['far2'] == 1 and sum([row['far3'],row['far4']]) == 0:\n",
    "        return 2\n",
    "    elif row['far3'] == 1 and row['far4'] == 0:\n",
    "        return 3\n",
    "    elif row['far4'] == 1:\n",
    "        return 4\n",
    "    \n",
    "def log_far_levels(dfx):\n",
    "    f = len(dfx)\n",
    "    f0 = len(dfx[dfx['FAR Level']==0])\n",
    "    print('Not Far and remote in InfoGroup file:',str(((f0/f) * 100))+'%',file=logfile)\n",
    "    print('Distribution of FAR types for enterprises with non-zero FAR Level:',file=logfile)\n",
    "    df = dfx[dfx['FAR Level']>0]\n",
    "    f = len(df)\n",
    "    f1 = len(df[df['FAR Level'] == 1])\n",
    "    print('\\tFAR Level 1:',str(((f1/f) * 100))+'%',file=logfile)\n",
    "    f2 = len(df[df['FAR Level'] == 2])\n",
    "    print('\\tFAR Level 2:',str(((f2/f) * 100))+'%',file=logfile)\n",
    "    f3 = len(df[df['FAR Level'] == 3])\n",
    "    print('\\tFAR Level 3:',str(((f3/f) * 100))+'%',file=logfile)\n",
    "    f4 = len(df[df['FAR Level'] == 4])\n",
    "    print('\\tFAR Level 4:',str(((f4/f) * 100))+'%',file=logfile)\n",
    "    \n",
    "def showtime(num):\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(str(num),'  ',dt_string)\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference datasets and derived data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T22:49:49.957825Z",
     "start_time": "2020-07-30T22:49:40.936074Z"
    }
   },
   "outputs": [],
   "source": [
    "# Census relationship files: CBSAs to counties\n",
    "cbsa_df = pd.read_csv(f'/home/tflory/notebooks/InfoGroup/rurality/Relationship_Files/cbsa-county-relationships-2017.csv',usecols=['STCOU','CBSA','LSAD'])\n",
    "cbsa_df.fillna('-9',inplace=True)\n",
    "cbsa_df.rename(columns={'STCOU':'FIPS Code'},inplace=True)\n",
    "cbsa_df['FIPS Code'] = cbsa_df['FIPS Code'].astype(int)\n",
    "\n",
    "# Census relationship files: Urban Areas to CBSAs\n",
    "relationship = '/home/tflory/notebooks/InfoGroup/rurality/Relationship_Files/Urban_Area_to_Metro_Micro_Area_utf-8.csv'\n",
    "rel = pd.read_csv(relationship)\n",
    "rel['UA Type'] = rel['UANAME'].apply(ua_type)\n",
    "# Make a list of lists of all the UA/CBSA pairs in the relationship file.\n",
    "pairs = rel[['UA','CBSA']].values.tolist()\n",
    "# Create dict from list of lists. pairs[0] is UA, pairs[1] is CBSA.\n",
    "# The result is a dict with keys of UAs and values of lists of CBSAs that are\n",
    "# associated with each UA in the relationship file\n",
    "uas = {}\n",
    "for pair in pairs:\n",
    "    try:\n",
    "        uas[pair[0]].append(pair[1])\n",
    "    except:\n",
    "        uas[pair[0]] = [pair[1]]\n",
    "        \n",
    "ua_keys = dict.fromkeys(uas.keys())\n",
    "for k,v in uas.items():\n",
    "    if len(v) > 1:\n",
    "        if all_nines(v):\n",
    "            ua_keys[k] = 'rural-multi'\n",
    "        elif 99999 in v:\n",
    "            ua_keys[k] = 'partly rural'\n",
    "        elif k == 99999:\n",
    "            ua_keys[k] = 'unknown'\n",
    "        else:\n",
    "            ua_keys[k]= 'urban-multi'\n",
    "    else:\n",
    "        if v[0] == 99999:\n",
    "            ua_keys[k] = 'rural-single'\n",
    "        else:\n",
    "            ua_keys[k] = 'urban-single'\n",
    "        \n",
    "# Create a dict with the UA as key and the UA Type as value.\n",
    "ua_type_dict = {}\n",
    "for v in rel[['UA','UA Type']].values.tolist():\n",
    "    ua_type_dict[v[0]] = v[1]\n",
    "    \n",
    "# FORHP list of 2300+ rural census tracts\n",
    "rural_tracts = []\n",
    "with open('/InfoGroup/data/rurality/tract_data.txt','r') as fin:\n",
    "    for line in fin:\n",
    "        if line[0] != chr(32):\n",
    "            continue\n",
    "        else:\n",
    "            line = line.strip()\n",
    "            try:\n",
    "                if line[0].isnumeric(): \n",
    "                    rural_tracts.append(line)\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "# Census Bureau: all census tracts, 2010 slightly updated in 2017\n",
    "tracts_file = \\\n",
    "      '/home/tflory/notebooks/InfoGroup/rurality/points-in-polygons/data/all_tracts.csv'\n",
    "tracts = pd.read_csv(tracts_file,dtype=object,usecols=['GEOID','rural_tract'])\n",
    "\n",
    "# ERS: Frontier and Remote census data tracts\n",
    "far_file = '/InfoGroup/data/rurality/FARcodesZIPdata2010WithAKandHI.csv'\n",
    "df_far = pd.read_csv(far_file,dtype=object)\n",
    "df_far['ZIP'] = df_far['ZIP'].apply(lambda x: x.zfill(5) if len(x) < 5 == 0 else x)\n",
    "df_zip = df_far[['ZIP','far1','far2','far3','far4']].copy()\n",
    "df_zip[['far1','far2','far3','far4']] = df_zip[['far1','far2','far3','far4']].astype(int)\n",
    "df_zip['FAR Level'] = df_zip.apply(farlevel,axis=1)\n",
    "df_zip = df_zip.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T22:49:49.963952Z",
     "start_time": "2020-07-30T22:49:49.960254Z"
    }
   },
   "outputs": [],
   "source": [
    "# open log file\n",
    "logfile = open('all_steps.log','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T02:24:46.288112Z",
     "start_time": "2020-07-30T22:49:49.966079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start    30/07/2020 17:49:49\n",
      "extract_and_correct    30/07/2020 18:18:45\n",
      "CBSA_partition    30/07/2020 18:20:40\n",
      "inline concat    30/07/2020 18:21:10\n",
      "\t5-0    30/07/2020 18:37:45\n",
      "\t5-1    30/07/2020 18:37:51\n",
      "\t5-2    30/07/2020 18:37:58\n",
      "\t5-3    30/07/2020 19:02:29\n",
      "\t5-4    30/07/2020 19:28:43\n",
      "\t5-5    30/07/2020 19:28:46\n",
      "new_vars (5)    30/07/2020 19:28:46\n",
      "\t6-0    30/07/2020 19:30:10\n",
      "\t6-1    30/07/2020 19:32:00\n",
      "\t6-2\n",
      "    30/07/2020 19:36:51\n",
      "merges_and_output (6)    30/07/2020 19:36:56\n",
      "start    30/07/2020 19:36:56\n",
      "extract_and_correct    30/07/2020 20:05:47\n",
      "CBSA_partition    30/07/2020 20:07:39\n",
      "inline concat    30/07/2020 20:08:10\n",
      "\t5-0    30/07/2020 20:24:43\n",
      "\t5-1    30/07/2020 20:24:48\n",
      "\t5-2    30/07/2020 20:24:55\n",
      "\t5-3    30/07/2020 20:49:53\n",
      "\t5-4    30/07/2020 21:16:36\n",
      "\t5-5    30/07/2020 21:16:40\n",
      "new_vars (5)    30/07/2020 21:16:40\n",
      "\t6-0    30/07/2020 21:18:03\n",
      "\t6-1    30/07/2020 21:19:45\n",
      "\t6-2\n",
      "    30/07/2020 21:24:41\n",
      "merges_and_output (6)    30/07/2020 21:24:46\n"
     ]
    }
   ],
   "source": [
    "for yr in range(1997,2015):\n",
    "    showtime('start')\n",
    "    df = extract_and_correct()\n",
    "    showtime('extract_and_correct')\n",
    "    (urban,rural,corrected) = CBSA_partition(df)\n",
    "    showtime('CBSA_partition')\n",
    "    final_df = pd.concat([urban,rural,corrected],ignore_index=True)\n",
    "    showtime('inline concat')\n",
    "    final_df = new_vars(final_df)\n",
    "    showtime('new_vars (5)')\n",
    "    merges_and_output(final_df)\n",
    "    showtime('merges_and_output (6)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T02:24:46.294347Z",
     "start_time": "2020-07-31T02:24:46.290543Z"
    }
   },
   "outputs": [],
   "source": [
    "logfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
